<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8"> 
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Rongfei" />
        <meta name="copyright" content="Rongfei" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="scrapy, python, " />

<meta property="og:title" content="scrapy使用 "/>
<meta property="og:url" content="/drafts/scrapyshi-yong.html" />
<meta property="og:description" content="" />
<meta property="og:site_name" content="LearnDevOps" />
<meta property="og:article:author" content="Rongfei" />
<meta property="og:article:published_time" content="2016-07-15T00:00:00+08:00" />
<meta name="twitter:title" content="scrapy使用 ">
<meta name="twitter:description" content="">

        <title>scrapy使用  · LearnDevOps
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="/theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/css/custom.css" media="screen">
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container-fluid">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="/"><span class=site-name>LearnDevOps</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="">Home</a></li>
                            <li ><a href="/pages/about-me.html">About Me</a></li>
                            <li ><a href="/categories.html">Categories</a></li>
                            <li ><a href="/tags.html">Tags</a></li>
                            <li ><a href="/archives.html">Archives</a></li>
                            <li><form class="navbar-search" action="/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article>
<div class="row-fluid">
    <header class="page-header span10 offset2">
    <h1><a href="/drafts/scrapyshi-yong.html"> scrapy使用  </a></h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">

            
            <!-- Status: published -->

<h1 id="1-scrapy">1. 安装scrapy 部署工具</h1>
<p>pip install scrapyd scrapyd-client python-scrapyd-api
curl</p>
<h1 id="2">2. 修改项目文件</h1>
<p>[deploy:localhost] #:后面是主机名
  url = http://localhost:6800/ #scrapd 服务器地址
  project = qixin</p>
<h1 id="3">3. 部署命令</h1>
<p>scrapyd-deploy -l 列出所有可用的scrapyd服务器
  scrapyd-deploy -L localhost  列出主机上的scrapy项目
  scrapyd-deploy example 把项目打包成egg并部署到scrapy服务器example</p>
<ul>
<li>发布项目
    格式： scrapyd-deploy target -p project
    target: 主机地址如 localhost
    project: 工程名称</li>
</ul>
<p>scrapyd-deploy  localhost -p qixin
   scrapyd-deploy  localhost -p wdzj</p>
<ul>
<li>查看部署信息
   http://localhost:6800/
   &hellip;
   Available projects: qixin, wdzj
   &hellip;</li>
</ul>
<h1 id="4">4. 调度爬虫</h1>
<p>curl http://localhost:6800/schedule.json -d project=default -d spider=somespider</p>
<p>运行爬虫：
  curl http://localhost:6800/schedule.json -d project=qixin -d spider=qixin-search
  project: 是工程名称
  spider: 是工程中的其中一个spider名称</p>
<p>取消爬虫：
  curl http://localhost:6800/cancel.json -d project=myproject -d job=6487ec79947edab326d6db28a2d86511e8247444</p>
<div class="codehilite" style="background: #f8f8f8"><pre style="line-height: 125%">调度爬虫
curl http://localhost:6800/schedule.json -d project=myproject -d spider=somespider

# 带上参数
curl http://localhost:6800/schedule.json -d project=myproject -d spider=somespider -d setting=DOWNLOAD_DELAY=2 -d arg1=val1

取消
curl http://localhost:6800/cancel.json -d project=myproject -d job=6487ec79947edab326d6db28a2d86511e8247444

列出项目
curl http://localhost:6800/listprojects.json

列出版本
curl http://localhost:6800/listversions.json?project=myproject

列出爬虫
curl http://localhost:6800/listspiders.json?project=myproject

列出job
curl http://localhost:6800/listjobs.json?project=myproject

删除版本
curl http://localhost:6800/delversion.json -d project=myproject -d version=r99

删除项目
curl http://localhost:6800/delproject.json -d project=myproject
</pre></div>


<h1 id="_1">配置文件</h1>
<div class="codehilite" style="background: #f8f8f8"><pre style="line-height: 125%">scrapyd启动的时候会自动搜索配置文件，配置文件的加载顺序为
/etc/scrapyd/scrapyd.conf
/etc/scrapyd/conf.d/*
scrapyd.conf
~/.scrapyd.conf
</pre></div>


<h1 id="scrapy">scrapy爬虫类</h1>
<ol>
<li>
<p>scrapy.spider.Spider
    Spider是最简单的spider。每个其他的spider必须继承自该类(包括Scrapy自带的其他spider以及您自己编写的spider)。 Spider并没有提供什么特殊的功能。 其仅仅请求给定的 start_urls/start_requests ，并根据返回的结果(resulting responses)调用spider的 parse 方法。</p>
<p>1.1 name 属性
定义spider名字的字符串(string)。spider的名字定义了Scrapy如何定位(并初始化)spider，所以其必须是唯一的。 不过您可以生成多个相同的spider实例(instance)，这没有任何限制。 name是spider最重要的属性，而且是必须的。
如果该spider爬取单个网站(single domain)，一个常见的做法是以该网站(domain)(加或不加 后缀 )来命名spider。 例如，如果spider爬取 mywebsite.com ，该spider通常会被命名为 mywebsite 。</p>
<p>1.2 allowed_domains 属性
可选。包含了spider允许爬取的域名(domain)列表(list)。 当 OffsiteMiddleware 启用时，域名不在列表中的URL不会被跟进。</p>
<p>1.3 start_urls
URL列表。当没有制定特定的URL时，spider将从该列表中开始进行爬取。 因此，第一个被获取到的页面的URL将是该列表之一。 后续的URL将会从获取到的数据中提取。</p>
<p>1.4 start_requests()
该方法必须返回一个可迭代对象(iterable)。该对象包含了spider用于爬取的第一个Request。 当spider启动爬取并且未制定URL时，该方法被调用。 当指定了URL时，make_requests_from_url() 将被调用来创建Request对象。 该方法仅仅会被Scrapy调用一次，因此您可以将其实现为生成器。</p>
<p>该方法的默认实现是使用 start_urls 的url生成Request。
如果您想要修改最初爬取某个网站的Request对象，您可以重写(override)该方法。 例如，如果您需要在启动时以POST登录某个网站，你可以这么写:
```
def start_requests(self):
    return [scrapy.FormRequest(&ldquo;http://www.example.com/login&rdquo;,
                               formdata={&lsquo;user&rsquo;: &lsquo;john&rsquo;, &lsquo;pass&rsquo;: &lsquo;secret&rsquo;},
                               callback=self.logged_in)]</p>
<p>def logged_in(self,response):
    # here you would extract links to follow and return Requests for
    # each of them, with another callback
    pass
    ```
  1.5 make_requests_from_url(url)
  该方法接受一个URL并返回用于爬取的 Request 对象。 该方法在初始化request时被 start_requests() 调用，也被用于转化url为request。</p>
</li>
</ol>
<p>默认未被复写(overridden)的情况下，该方法返回的Request对象中， parse() 作为回调函数，dont_filter参数也被设置为开启。 (详情参见 Request).</p>
<p>1.6 parse(response)
  当response没有指定回调函数时，该方法是Scrapy处理下载的response的默认方法。
  parse 负责处理response并返回处理的数据以及(/或)跟进的URL。 Spider 对其他的Request的回调函数也有相同的要求。
  该方法及其他的Request回调函数必须返回一个包含 Request 及(或) Item 的可迭代的对象。
  参数:   response (Response) – 用于分析的response</p>
<p>1.7 log(message[, level, component])
    使用 scrapy.log.msg() 方法记录(log)message。 log中自动带上该spider的 name 属性。 更多数据请参见 Logging 。</p>
<p>1.8 closed(reason)
    当spider关闭时，该函数被调用。 该方法提供了一个替代调用signals.connect()来监听 spider_closed 信号的快捷方式。</p>
<p>## Spider简单例子:
  ```
  import scrapy
  class MySpider(scrapy.Spider):
      name = &lsquo;example.com&rsquo;
      allowed_domains = [&lsquo;example.com&rsquo;]
      start_urls = [
          &lsquo;http://www.example.com/1.html&rsquo;,
          &lsquo;http://www.example.com/2.html&rsquo;,
          &lsquo;http://www.example.com/3.html&rsquo;,
      ]</p>
<div class="codehilite" style="background: #f8f8f8"><pre style="line-height: 125%">  def parse(self, response):
      self.log(&#39;A response from %s just arrived!&#39; % response.url)
```
</pre></div>


<p>## 另一个在单个回调函数中返回多个Request以及Item的例子:
  ```
  import scrapy
  from myproject.items import MyItem</p>
<p>class MySpider(scrapy.Spider):
      name = &lsquo;example.com&rsquo;
      allowed_domains = [&lsquo;example.com&rsquo;]
      start_urls = [
          &lsquo;http://www.example.com/1.html&rsquo;,
          &lsquo;http://www.example.com/2.html&rsquo;,
          &lsquo;http://www.example.com/3.html&rsquo;,
      ]</p>
<div class="codehilite" style="background: #f8f8f8"><pre style="line-height: 125%">  def parse(self, response):
      sel = scrapy.Selector(response)
      for h3 in response.xpath(&#39;//h3&#39;).extract():
          yield MyItem(title=h3)

      for url in response.xpath(&#39;//a/@href&#39;).extract():
          yield scrapy.Request(url, callback=self.parse)
</pre></div>


<p>```
  2. scrapy.spider.Spider</p>
            
            
            <hr/>
        </div>
        <section>
        <div class="span2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time pubdate="pubdate" datetime="2016-07-15T00:00:00+08:00"> 7 15, 2016</time>
            <h4>Category</h4>
            <a class="category-link" href="/categories.html#python-ref">python</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="/tags.html#scrapy-ref">scrapy
</a></li>
            </ul>
        </div>
        </section>
</div>

<!-- 多说评论框 start -->
<div class="ds-thread" data-thread-key="scrapy使用" data-title="scrapy使用" data-url="http://blog.pyshell.cn/drafts/scrapyshi-yong.html"></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"devops"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
<!-- 多说公共JS代码 end -->
<!-- baidu tongji   -->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?b6fb6111b823b418109202b642b4a62b";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<!-- baidu tongji  end -->

</article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
        <div id="push"></div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>            <script src="http://code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    
    </body>
    <!-- Theme: Elegant built for Pelican
    License : http://oncrashreboot.com/pelican-elegant -->
</html>